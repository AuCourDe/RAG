# JAK DZIAÅA OLLAMA - WyjaÅ›nienie

## ğŸ¤” Czym jest Ollama?

**Ollama** to **osobny serwis** (daemon), ktÃ³ry dziaÅ‚a w tle na systemie Linux i udostÄ™pnia modele LLM przez API HTTP.

---

## ğŸ”„ Jak to dziaÅ‚a w RAG2?

### 1. **Ollama dziaÅ‚a NIEZALEÅ»NIE od aplikacji RAG**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP API       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Service     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   RAG App        â”‚
â”‚  (localhost:11434)  â”‚                      â”‚   (Streamlit)    â”‚
â”‚                     â”‚                      â”‚                  â”‚
â”‚  - gemma3:12b       â”‚                      â”‚  - query()       â”‚
â”‚  - mistral          â”‚                      â”‚  - generate()    â”‚
â”‚  - llama2           â”‚                      â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Ollama uruchamianie**

Ollama **NIE jest uruchamiany przez aplikacjÄ™ RAG**. Musi byÄ‡ uruchomiony **rÄ™cznie** lub jako **systemd service**.

#### Opcja A: Uruchomienie rÄ™czne
```bash
# SprawdÅº czy dziaÅ‚a
ollama list

# Uruchom serwis (jeÅ›li nie dziaÅ‚a)
ollama serve

# W tle (daemon)
nohup ollama serve > /dev/null 2>&1 &
```

#### Opcja B: Systemd service (automatyczne uruchomienie)
```bash
# Ollama instaluje siÄ™ jako systemd service
systemctl status ollama

# Uruchom
sudo systemctl start ollama

# WÅ‚Ä…cz auto-start przy bootowaniu
sudo systemctl enable ollama
```

### 3. **Sprawdzenie stanu Ollama**

```bash
# SprawdÅº czy serwis dziaÅ‚a
curl http://localhost:11434/api/tags

# Lista zaÅ‚adowanych modeli
ollama list

# SprawdÅº zaÅ‚adowany model
ollama ps
```

---

## ğŸ’» Co robi aplikacja RAG?

Aplikacja RAG **tylko Å‚Ä…czy siÄ™** do Ollama przez HTTP, **nie uruchamia** Ollama.

### W `model_provider.py`:

```python
class OllamaProvider:
    def __init__(self, model_name="gemma3:12b", base_url="http://127.0.0.1:11434"):
        self.base_url = base_url  # Adres serwisu Ollama
        self.model_name = model_name
        
    def generate(self, prompt, **kwargs):
        # WysyÅ‚a request HTTP do Ollama
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model_name,
                "prompt": prompt,
                ...
            }
        )
        return response.json()
```

### PrzepÅ‚yw:
1. UÅ¼ytkownik zadaje pytanie w aplikacji RAG
2. RAG znajduje odpowiednie dokumenty (vector search)
3. RAG przygotowuje prompt z kontekstem
4. RAG **wysyÅ‚a request HTTP** do Ollama (port 11434)
5. Ollama generuje odpowiedÅº uÅ¼ywajÄ…c modelu gemma3:12b
6. RAG otrzymuje odpowiedÅº i wyÅ›wietla uÅ¼ytkownikowi

---

## ğŸš€ Ollama - Start automatyczny

### SprawdÅº czy Ollama dziaÅ‚a przy starcie systemu:

```bash
# Status
systemctl status ollama

# JeÅ›li nie dziaÅ‚a, wÅ‚Ä…cz
sudo systemctl enable ollama
sudo systemctl start ollama
```

### Logi Ollama:
```bash
# Logi systemd
journalctl -u ollama -f

# Lub sprawdÅº proces
ps aux | grep ollama
```

---

## ğŸ“Š ZarzÄ…dzanie modelami w Ollama

### Pobieranie modeli:
```bash
# Pobierz model
ollama pull gemma3:12b
ollama pull mistral
ollama pull llama2

# Lista pobranych
ollama list
```

### Modele sÄ… przechowywane w:
```bash
~/.ollama/models/
```

### UsuniÄ™cie modelu:
```bash
ollama rm gemma3:12b
```

---

## âš™ï¸ Konfiguracja w RAG

### W `auth_config.json`:
```json
{
  "ollama": {
    "model": "gemma3:12b",
    "base_url": "http://127.0.0.1:11434"
  }
}
```

### Zmiana modelu:
W aplikacji RAG (Streamlit UI):
- Sidebar â†’ "Model LLM" â†’ Wybierz model z listy

---

## ğŸ”§ Troubleshooting

### Problem: "Connection refused" lub "Ollama niedostÄ™pny"

**Przyczyna:** Ollama nie dziaÅ‚a

**RozwiÄ…zanie:**
```bash
# Uruchom Ollama
ollama serve

# Lub jako systemd
sudo systemctl start ollama

# SprawdÅº
curl http://localhost:11434/api/tags
```

### Problem: Model nie jest dostÄ™pny

**Przyczyna:** Model nie zostaÅ‚ pobrany

**RozwiÄ…zanie:**
```bash
ollama pull gemma3:12b
```

### Problem: Ollama zuÅ¼ywa duÅ¼o VRAM

**Przyczyna:** Model jest zaÅ‚adowany w pamiÄ™ci GPU

**RozwiÄ…zanie:**
```bash
# WyÅ‚aduj model
ollama stop gemma3:12b

# Lub restartuj Ollama
sudo systemctl restart ollama
```

---

## ğŸ“ˆ WydajnoÅ›Ä‡

### Ollama auto-zarzÄ…dza VRAM:
- **Pierwszy request:** Åaduje model do VRAM (~5-10s)
- **Kolejne requesty:** Model juÅ¼ w pamiÄ™ci (szybkie, <1s)
- **Idle:** Po kilku minutach bezczynnoÅ›ci wyÅ‚adowuje model

### Monitoring:
```bash
# GPU usage
nvidia-smi

# Ollama proces
ollama ps
```

---

## ğŸ¯ Podsumowanie

| Pytanie | OdpowiedÅº |
|---------|-----------|
| **Czy Ollama dziaÅ‚a caÅ‚y czas?** | TAK, jako service w tle (daemon) |
| **Czy RAG uruchamia Ollama?** | NIE, tylko Å‚Ä…czy siÄ™ przez HTTP |
| **Czy Ollama musi dziaÅ‚aÄ‡?** | TAK, inaczej RAG nie wygeneruje odpowiedzi |
| **Gdzie sÄ… modele?** | `~/.ollama/models/` |
| **Jak zmieniÄ‡ model?** | W UI Streamlit lub `auth_config.json` |
| **Czy moÅ¼na wyÅ‚Ä…czyÄ‡ Ollama?** | TAK: `sudo systemctl stop ollama` |

---

## ğŸŒ Alternatywy dla Ollama

Aplikacja RAG obsÅ‚uguje teÅ¼:
- **OpenAI API** (GPT-4, GPT-3.5) - wymaga klucza API
- **Inne kompatybilne API** (LM Studio, text-generation-webui)

Konfiguracja w `model_provider.py` i `auth_config.json`.

---

**âœ… Wnioski:**
- Ollama = osobny serwis, dziaÅ‚a w tle
- RAG = klient HTTP, wysyÅ‚a zapytania do Ollama
- Ollama moÅ¼na uruchomiÄ‡ raz i zostawiÄ‡ (auto-zarzÄ…dza VRAM)

