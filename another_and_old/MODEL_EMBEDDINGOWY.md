# ğŸ“Š Model do embeddingÃ³w

## Aktualny model: `intfloat/multilingual-e5-large`

### Specyfikacja:

| Parametr | WartoÅ›Ä‡ |
|----------|---------|
| **Nazwa** | intfloat/multilingual-e5-large |
| **Typ** | Sentence Transformer |
| **Wymiar wektora** | **1024** |
| **Max dÅ‚ugoÅ›Ä‡** | 512 tokenÃ³w (~500 znakÃ³w) |
| **JÄ™zyki** | Multilingual (w tym **polski**) |
| **UrzÄ…dzenie** | **GPU (CUDA)** |
| **Rozmiar modelu** | ~560 MB |

### WydajnoÅ›Ä‡ na RTX 3060:

```
âœ… GPU: NVIDIA RTX 3060 12GB
âœ… CUDA: 12.8
âœ… Batch size: 32 fragmenty
âœ… Czas: ~0.02 sekundy per fragment
âœ… Throughput: ~50 fragmentÃ³w/sekundÄ™
```

---

## ğŸ“ˆ Dlaczego wÅ‚aÅ›nie ten model?

### 1. **Multilingual**
- Wspiera **100+ jÄ™zykÃ³w**, w tym polski
- Trenowany na multilingual data
- Najlepszy dla dokumentÃ³w po polsku

### 2. **Wysoka jakoÅ›Ä‡**
- State-of-the-art dla zadaÅ„ multilingual
- DoskonaÅ‚e wyniki w podobieÅ„stwie semantycznym
- Ranking: Top 3 na MTEB Leaderboard (multilingual)

### 3. **Optymalny rozmiar**
- 1024 wymiary - Å›wietny balans precision/performance
- Nie za duÅ¼y (nie overfit), nie za maÅ‚y (dobra precyzja)

### 4. **GPU-optimized**
- PeÅ‚ne wsparcie dla CUDA
- Szybkie batch processing
- Efektywne wykorzystanie VRAM

---

## ğŸ”„ Alternatywne modele

### JeÅ›li chcesz zmieniÄ‡ model:

| Model | Wymiary | JÄ™zyki | SzybkoÅ›Ä‡ | JakoÅ›Ä‡ |
|-------|---------|--------|----------|--------|
| **intfloat/multilingual-e5-large** (AKTUALNY) | 1024 | Multi | â­â­â­ | â­â­â­â­â­ |
| intfloat/multilingual-e5-base | 768 | Multi | â­â­â­â­ | â­â­â­â­ |
| intfloat/multilingual-e5-small | 384 | Multi | â­â­â­â­â­ | â­â­â­ |
| paraphrase-multilingual-mpnet-base-v2 | 768 | Multi | â­â­â­â­ | â­â­â­ |
| sdadas/mmlw-roberta-large | 1024 | PL/Multi | â­â­â­ | â­â­â­â­ |

### Jak zmieniÄ‡ model:

W pliku `rag_system.py`, linia 439:
```python
self.model = SentenceTransformer('intfloat/multilingual-e5-large')
```

ZmieÅ„ na np.:
```python
self.model = SentenceTransformer('intfloat/multilingual-e5-base')  # Szybszy
```

âš ï¸ **UWAGA:** Po zmianie modelu musisz przeindeksowaÄ‡ wszystkie dokumenty!

---

## ğŸ§® Jak dziaÅ‚a embedding?

### Proces:

```
Tekst â†’ Tokenizacja â†’ Model â†’ Wektor 1024D
```

### PrzykÅ‚ad:

```python
Tekst: "Kto popeÅ‚nia przestÄ™pstwo podlega karze"
       â†“
Embedding: [0.020, -0.009, -0.009, -0.065, 0.017, ...]
           (1024 liczby zmiennoprzecinkowe)
```

### Co reprezentuje wektor?

- **Semantyczne znaczenie** tekstu
- Podobne teksty â†’ podobne wektory
- OdlegÅ‚oÅ›Ä‡ miÄ™dzy wektorami = podobieÅ„stwo semantyczne

### Wyszukiwanie:

```python
Zapytanie: "Jakie kary za kradzieÅ¼?"
Embedding zapytania: [0.034, -0.012, ...]

PorÃ³wnanie z bazÄ… (3,483 wektory):
  Fragment 1: odlegÅ‚oÅ›Ä‡ 0.15 â† NAJBLIÅ»SZY! âœ…
  Fragment 2: odlegÅ‚oÅ›Ä‡ 0.32
  Fragment 3: odlegÅ‚oÅ›Ä‡ 0.89
  ...
```

---

## ğŸ“Š Statystyki z Twojej bazy:

```
âœ… FragmentÃ³w: 3,483
âœ… EmbeddingÃ³w: 3,483 wektory Ã— 1024 wymiary
âœ… Rozmiar: ~14 MB (same embeddingi)
âœ… Czas tworzenia: ~77 sekund (caÅ‚oÅ›Ä‡)
âœ… Czas per fragment: ~0.022 sekundy
```

---

## ğŸ” JakoÅ›Ä‡ embeddingÃ³w

Model `multilingual-e5-large` jest wytrenowany na:

- **1 miliard** par (pytanie, odpowiedÅº)
- **100+ jÄ™zykÃ³w**
- Dane z Wikipedia, StackExchange, Reddit
- Fine-tuned na similarity tasks

**Wynik:** DoskonaÅ‚e rozpoznawanie podobieÅ„stwa semantycznego w jÄ™zyku polskim! âœ…
